{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01327f7e",
   "metadata": {},
   "source": [
    "# Advanced Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d40e9",
   "metadata": {},
   "source": [
    "Adjust code according to your variables, these lines wont run as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful imports start notebook with these\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in datasets\n",
    "df=pd.read_csv('',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e830417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split variables in dataset\n",
    "y = df['ZAR/USD']\n",
    "X = df.drop('ZAR/USD', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split predictors and response, noticed the code is sligghtly different for multiple variables. Not sure if this matters \n",
    "#or is just another way of coding it\n",
    "X = df.drop(['mpg'], axis=1)\n",
    "y = df['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48320a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e14be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create scaler object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119dd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1630e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is not split with scaled data, change variables accordingly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a74c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is split with scaled values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b299c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe based on scale data\n",
    "X_standardise = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "#view standardised set\n",
    "X_standardise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training linear regression model\n",
    "# Import the linear regression module\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582229ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear regression module\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data (also known as training the model)\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the intercept, or y-cut, of our linear model\n",
    "a = float(lm.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coefficient, or gradient, of our linear model\n",
    "b = lm.coef_\n",
    "###################################################\n",
    "\n",
    "beta_0 = float(lm.intercept_)\n",
    "# extract model coeffs for mlr\n",
    "beta_js = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])\n",
    "\n",
    "#create a df based on beta_js\n",
    "\n",
    "beta_js = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Slope:\\t\\t\", b)\n",
    "print(\"Intercept:\\t\", float(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11390ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the values that fall along our regression line\n",
    "gen_y = lm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import metrics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "# Calculate the mean-squared-error\n",
    "print('MSE:', metrics.mean_squared_error(y_train, gen_y))\n",
    "# Calculate the R-squared metric\n",
    "print('R_squared:', metrics.r2_score(y_train, gen_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values of y from x, using the linear model\n",
    "gen_y_test = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5414f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing:\")\n",
    "print('MSE:', metrics.mean_squared_error(y_test, gen_y_test))\n",
    "print('R_squared:', metrics.r2_score(y_test, gen_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebdb6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Mean squared error is higher on the test set than the train set, indicating poor predictive accuracy, and R-squared is lower on the test set, indicating a worse fit on the test set.\n",
    "\n",
    "These results indicate a concept in machine learning model fitting known as overfitting. This is a phenomenon where there is:\n",
    "\n",
    "A discrepancy between the performance of the model on train and on test sets; and\n",
    "An inability of the model to generalise to data it has not seen before.\n",
    "The term comes from the fact that the model fits too well, or overfits, the training data, and does not fit well, or underfits, the testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary of results\n",
    "# dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_train, slr.predict(X_train[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_train, lm.predict(X_train))\n",
    "                    },\n",
    "                'Test MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_test, lm.predict(X_test))\n",
    "                    },\n",
    "                'Test RMSE':\n",
    "                    {\n",
    "                        \"SLR\": math.sqrt(metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']]))),\n",
    "                        \"MLR\": math.sqrt(metrics.mean_squared_error(y_test, lm.predict(X_test)))\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from dictionary above\n",
    "results_df = pd.DataFrame(data=results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbc7aa",
   "metadata": {},
   "source": [
    "Probably missed out stuff but putting more Classification code as it consists of most of the marks. Did not include other models and Ridge and Lasso Regularisation. Ensemble methods not included as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson Regression, not sure if this is correlation\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Build a dictionary of correlation coefficients and p-values\n",
    "dict_cp = {}\n",
    "\n",
    "column_titles = [col for col in corrs.index if col!= 'Loan_Size']\n",
    "for col in column_titles:\n",
    "    p_val = round(pearsonr(df_dummies[col], df_dummies['Loan_Size'])[1],6)\n",
    "    dict_cp[col] = {'Correlation_Coefficient':corrs[col],\n",
    "                    'P_Value':p_val}\n",
    "\n",
    "df_cp = pd.DataFrame(dict_cp).T\n",
    "df_cp_sorted = df_cp.sort_values('P_Value')\n",
    "df_cp_sorted[df_cp_sorted['P_Value']<0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefb1f1",
   "metadata": {},
   "source": [
    "# Classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35991765",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068cbb7c",
   "metadata": {},
   "source": [
    "Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "y = df['insurance_claim']\n",
    "\n",
    "# features\n",
    "X = df.drop('insurance_claim', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Features\n",
    "X_transformed = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d535ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920be1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function uses regular expressions to remove html characters,\n",
    "    punctuation, numbers and any extra white space from each text\n",
    "    and then converts them to lowercase.\n",
    "\n",
    "    Input:\n",
    "    text: original text\n",
    "          datatype: string\n",
    "\n",
    "    Output:\n",
    "    texts: modified text\n",
    "           datatype: string\n",
    "    \"\"\"\n",
    "    # replace the html characters with \" \"\n",
    "    text=re.sub('<.*?>', ' ', text)\n",
    "#    Removal of numbers\n",
    "#    text = re.sub(r'\\d+', ' ', text)\n",
    "    # will replace newline with space\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    # will convert to lower case\n",
    "    text = text.lower()\n",
    "    # will split and join the words\n",
    "    text=' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the function to clean the tweets\n",
    "train['text'] = train['text'].apply(clean_text)\n",
    "test['text'] = test['text'].apply(clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95800dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '.txt' with 'text file'\n",
    "train[\"text\"] = train[\"text\"].str.replace(\".txt\", \" text file\")\n",
    "test[\"text\"] = test[\"text\"].str.replace(\".txt\", \" text file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Function for Model Building\n",
    "\n",
    "def models_building(classifiers, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    This function takes in a list of classifiers\n",
    "    and both the train and validation sets\n",
    "    and return a summary of F1-score and\n",
    "    processing time as a dataframe\n",
    "\n",
    "    Input:\n",
    "    classifiers: a list of classifiers to train\n",
    "                 datatype: list\n",
    "    X_train: independent variable for training\n",
    "             datatype: series\n",
    "    y_train: dependent variable for training\n",
    "             datatype: series\n",
    "    X_val: independent variable for validation\n",
    "           datatype: series\n",
    "    y_val: dependent variable for validation\n",
    "           datatype: series\n",
    "\n",
    "    Output:\n",
    "    model_summary: F1 Score for all the classifiers\n",
    "                   datatype: dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    models_summary = {}\n",
    "\n",
    "    # Pipeline to balance the classses and then to build the model\n",
    "    for clf in classifiers:\n",
    "        clf_text = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n",
    "                                                       max_df=0.9,\n",
    "                                                       ngram_range=(3, 6),\n",
    "                                                       analyzer='char')),\n",
    "                             ('clf', clf)])\n",
    "\n",
    "        # Logging the Execution Time for each model\n",
    "        start_time = time.time()\n",
    "        clf_text.fit(X_train, y_train)\n",
    "        predictions = clf_text.predict(X_val)\n",
    "        run_time = time.time()-start_time\n",
    "\n",
    "        # Output for each model\n",
    "        models_summary[clf.__class__.__name__] = {\n",
    "            'F1-Macro': metrics.f1_score(y_val,\n",
    "                                         predictions,\n",
    "                                         average='macro'),\n",
    "            'F1-Accuracy': metrics.f1_score(y_val, predictions,\n",
    "                                            average='micro'),\n",
    "            'F1-Weighted': metrics.f1_score(y_val,\n",
    "                                            predictions,\n",
    "                                            average='weighted'),\n",
    "            'Execution Time': run_time}\n",
    "\n",
    "    return pd.DataFrame.from_dict(models_summary, orient='index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11eb3d",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning on Most Performing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2942f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline for the gridsearch\n",
    "param_grid = {'alpha': [0.1, 1, 5, 10]}  # setting parameter grid\n",
    "\n",
    "HP_MNB = Pipeline([('tfidf', TfidfVectorizer(min_df=2,\n",
    "                                                max_df=0.9,\n",
    "                                                ngram_range=(3, 6),\n",
    "                                                analyzer='char')),\n",
    "                      ('mnb', GridSearchCV(MultinomialNB(),\n",
    "                                           param_grid=param_grid,\n",
    "                                           cv=5,\n",
    "                                           n_jobs=-1,\n",
    "                                           scoring='f1_weighted'))\n",
    "                      ])\n",
    "\n",
    "HP_MNB.fit(X_train, y_train)  # Fitting the model\n",
    "\n",
    "y_pred_mnb = HP_MNB.predict(X_val)  # predicting the fit on validation set\n",
    "\n",
    "print(classification_report(y_val, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution of the Classifiers\n",
    "\n",
    "classifiers_df = models_building(classifiers, X_train, y_train, X_val, y_val)\n",
    "ordered_df = classifiers_df.sort_values('F1-Macro', ascending=False)\n",
    "ordered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5a100",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed32972",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144a5ae",
   "metadata": {},
   "source": [
    "Pro-tip: in the multi-class case we referred to above, the LogisticRegression instance takes a simple argument which enables it to be used for 2+ classes: multi_class='ovr'. We'll revisit this later in the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff696e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10684b29",
   "metadata": {},
   "source": [
    "Advantages & Disadvantages of Logistic Regression\n",
    "Advantages\n",
    "\n",
    "Convenient probability scores for observations (probability of each outcome is transformed into a classification);\n",
    "Not a major issue if there is collinearity among features (much worse with linear regression).\n",
    "Disadvantages\n",
    "\n",
    "Can overfit when data is unbalanced (i.e.: we have far more observations in one class than the other);\n",
    "Doesn't handle large number of categorical variables well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204fcb9",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4743641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0: Malignant', '1: Benign']\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, pred_lm), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed15cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c639fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_test, pred_lm, target_names=['0: Malignant', '1: Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a48dc",
   "metadata": {},
   "source": [
    "### Overall Accuracy\n",
    "\n",
    "The results shown above lead us to our first classification metric: **overall accuracy**, which we calculate according to the following formula:\n",
    "\n",
    "$$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions} = \\frac{TP + \\space TN}{TP \\space + \\space TN \\space + \\space FP \\space + \\space FN}$$\n",
    "\n",
    "Our overall accuracy is calculated as follows:\n",
    "\n",
    "$$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions} = \\frac{70 + 36}{70 + 36 + 3 + 3} = 0.946$$\n",
    "\n",
    "At first glance this appears to a useful, catch-all metric which tells us everything we need to know about our model. The problem is that it lacks detail.\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "- We have 100 observations in our test dataset: 90 of them are labelled _No_ , the remaining 10 labelled _Yes_. \n",
    "\n",
    "- At prediction time, our model classifies all 100 observations to be in category _No_. Our model made 100 predictions, and got all 90 of the _No_ observations correct, giving it an overall accuracy of 90%!\n",
    "\n",
    "- Sounds good right? The problem is that the model got literally none of the _Yes_-labelled observations correct - 0/10! What if the _Yes_ cases were for patients have cancer, or a transaction that is fraudulent? Those are important results, and we would have missed all of them.\n",
    "\n",
    "- Hopefully, that has highlighted the importance of being accurate not just overall, but in each particular class too.\n",
    "\n",
    "Let's look at few metrics which are a little more comprehensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e64ac",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "When it predicts _yes_, how often is it correct? \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6d7d0",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "\n",
    "When the outcome is actually _yes_, how often do we predict it as such?\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192c4c6",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "\n",
    "Weighted average of precision and recall. \n",
    "\n",
    "$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c252c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aae258",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_test, pred_lm, target_names=['0: Malignant', '1: Benign']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61113697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
