{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01327f7e",
   "metadata": {},
   "source": [
    "# *Advanced Regression*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d40e9",
   "metadata": {},
   "source": [
    "Adjust code according to your variables, these lines wont run as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#useful imports start notebook with these\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in datasets\n",
    "df=pd.read_csv('',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e830417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split variables in dataset\n",
    "y = df['ZAR/USD']\n",
    "X = df.drop('ZAR/USD', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split predictors and response, noticed the code is sligghtly different for multiple variables. Not sure if this matters \n",
    "#or is just another way of coding it\n",
    "X = df.drop(['mpg'], axis=1)\n",
    "y = df['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48320a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data (i.e - used to standardize data)\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e14be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create scaler object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119dd32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaled version of the predictors (there is no need to scale the response)\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1630e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is not split with scaled data, change variables accordingly\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a74c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is split with scaled values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b299c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe based on scale data\n",
    "X_standardise = pd.DataFrame(X_scaled,columns=X.columns)\n",
    "\n",
    "#view standardised set\n",
    "X_standardise.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650cfb53-7c43-4c3b-a7bc-f926f9169e51",
   "metadata": {},
   "source": [
    "### Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training linear regression model\n",
    "# Import the linear regression module\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582229ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the linear regression module\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a94f2-c0b3-48a6-b58b-6cfe8123fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model object\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data (also known as training the model)\n",
    "lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da3837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the intercept, or y-cut, of our linear model\n",
    "a = float(lm.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cd25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coefficient, or gradient, of our linear model\n",
    "b = lm.coef_\n",
    "###################################################\n",
    "\n",
    "beta_0 = float(lm.intercept_)\n",
    "# extract model coeffs for mlr\n",
    "beta_js = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])\n",
    "\n",
    "#create a df based on beta_js\n",
    "\n",
    "beta_js = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Slope:\\t\\t\", b)\n",
    "print(\"Intercept:\\t\", float(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11390ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the values that fall along our regression line\n",
    "gen_y = lm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999b30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import metrics\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c02fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training:\")\n",
    "# Calculate the mean-squared-error\n",
    "print('MSE:', metrics.mean_squared_error(y_train, gen_y))\n",
    "# Calculate the R-squared metric\n",
    "print('R_squared:', metrics.r2_score(y_train, gen_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate values of y from x, using the linear model\n",
    "gen_y_test = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5414f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing:\")\n",
    "print('MSE:', metrics.mean_squared_error(y_test, gen_y_test))\n",
    "print('R_squared:', metrics.r2_score(y_test, gen_y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebdb6f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Mean squared error is higher on the test set than the train set, indicating poor predictive accuracy, and R-squared is lower on the test set, indicating a worse fit on the test set.\n",
    "\n",
    "These results indicate a concept in machine learning model fitting known as overfitting. This is a phenomenon where there is:\n",
    "\n",
    "A discrepancy between the performance of the model on train and on test sets; and\n",
    "An inability of the model to generalise to data it has not seen before.\n",
    "The term comes from the fact that the model fits too well, or overfits, the training data, and does not fit well, or underfits, the testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary of results\n",
    "# dictionary of results\n",
    "results_dict = {'Training MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_train, slr.predict(X_train[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_train, lm.predict(X_train))\n",
    "                    },\n",
    "                'Test MSE':\n",
    "                    {\n",
    "                        \"SLR\": metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']])),\n",
    "                        \"MLR\": metrics.mean_squared_error(y_test, lm.predict(X_test))\n",
    "                    },\n",
    "                'Test RMSE':\n",
    "                    {\n",
    "                        \"SLR\": math.sqrt(metrics.mean_squared_error(y_test, slr.predict(X_test[['disp']]))),\n",
    "                        \"MLR\": math.sqrt(metrics.mean_squared_error(y_test, lm.predict(X_test)))\n",
    "                    }\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ad1d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe from dictionary above\n",
    "results_df = pd.DataFrame(data=results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121660b-e05c-47d4-9967-faec101a49b2",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "#### NB!! Must scale data [ i.e ScandardScaler() ] before commencing with Ridge Regression!!\n",
    "\n",
    "Only pre-lim steps. See Linear Regression Code above for whole modelling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba586e-e555-4feb-a127-d45db9314d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the response\n",
    "X = df.drop('Target_column', axis=1)\n",
    "y = df['Target_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f9a27-3958-48ec-af1c-b19328814512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95359bb5-083d-43cf-a3b9-a35ddf2e1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardization object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ab66e-855d-488a-aac3-cc5ae9413b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save standardized features into new variable\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b788024-59da-450c-b868-5840272bdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train/test split module\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f439bdd8-5b2f-4cac-901b-ca0e87f186d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,\n",
    "                                                    y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=1,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5cefd2-6c85-4d5d-b189-520b993c64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ridge regression module from sklearn\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03355d-8bb6-4945-affe-dcd0d66932d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ridge model\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c76e4-fb49-46d0-8e93-ae2078360a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93ffcd-cc8b-4de5-a2db-268412edc89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the model intercept value\n",
    "b0 = float(ridge.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b4d158-bd5f-46a3-94b8-165debb72f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the model coefficient value\n",
    "coeff = pd.DataFrame(ridge.coef_, X.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506d395-779a-4372-861b-97b1126cf755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the intercept\n",
    "print(\"Intercept:\", float(b0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca20e1bc-b3da-4144-b308-26e219ed8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the coefficients\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e7bf0e-59ba-46db-8778-ecfb559a18f3",
   "metadata": {},
   "source": [
    "### LASSO Regression\n",
    "\n",
    "#### NB!! Must scale data [ i.e ScandardScaler() ] before commencing with LASSO Regression!!\n",
    "\n",
    "Only pre-lim steps. See Linear Regression Code above for whole modelling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565ffbd-f89b-45df-b8e9-7da9afedbbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features from the response\n",
    "X = df.drop('Target_column', axis=1)\n",
    "y = df['Target_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dabe7d-3ae6-4399-8555-69ff6a7d98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaling module\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397a133-3776-47fa-8995-269fec6675c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standardization object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce2dc2-aa88-4abf-a095-3a3f347967de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save standardized features into new variable\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe13367-80f8-4aeb-b524-5af335ac38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train/test split module\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecee63c-3f41-48e9-8e62-0edf8ce12e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled,\n",
    "                                                    y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=1,\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee685c-cea0-486b-a9c4-188e05c3e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LASSO module\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d8b1c-00af-423e-81c4-a09ae12f0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LASSO model object, setting alpha to 0.01\n",
    "lasso = Lasso(alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e912ab33-c56f-4c3b-8a84-7fcd5421b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LASSO model\n",
    "lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23a55e-437d-4838-b6f5-e6a0a5cb7011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract intercept from model\n",
    "intercept = float(lasso.interce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05675341-4b8e-42cb-a3a7-0ebf107ae23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coefficient from model\n",
    "coeff = pd.DataFrame(lasso.coef_, X.columns, columns=['Coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bab66f-f5ec-42fc-b9c8-727c73fb4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract intercept\n",
    "print(\"Intercept:\", float(intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514a6aa-5d30-4cf3-aa0e-3d016f1fce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the co-efficient values\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dbc7aa",
   "metadata": {},
   "source": [
    "Probably missed out stuff but putting more Classification code as it consists of most of the marks. Did not include other models and Ridge and Lasso Regularisation. Ensemble methods not included as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8427b27-4f0e-45cb-b52d-2b9706325e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd75e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson Regression, not sure if this is correlation\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Build a dictionary of correlation coefficients and p-values\n",
    "dict_cp = {}\n",
    "\n",
    "column_titles = [col for col in corrs.index if col!= 'Loan_Size']\n",
    "for col in column_titles:\n",
    "    p_val = round(pearsonr(df_dummies[col], df_dummies['Loan_Size'])[1],6)\n",
    "    dict_cp[col] = {'Correlation_Coefficient':corrs[col],\n",
    "                    'P_Value':p_val}\n",
    "\n",
    "df_cp = pd.DataFrame(dict_cp).T\n",
    "df_cp_sorted = df_cp.sort_values('P_Value')\n",
    "df_cp_sorted[df_cp_sorted['P_Value']<0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6cb904-cf5d-45eb-91a8-cc89c2621c4d",
   "metadata": {},
   "source": [
    "### Pearson Correlation \n",
    "\n",
    "The Pearson correlation measures the linear relationship between features and assumes that the features are normally distributed. Values range between -1 (Perfect Negative Correlation) and 1 (Perfect Positive Correlation). Below is the code to output a Pearson correlation table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336517c0-8d17-4258-bcb0-8f7476e5d7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886af009-7c56-4dd9-8241-a1ed6bb9785e",
   "metadata": {},
   "source": [
    "# *Statistics / EDA*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe0f4db-0d60-4851-92e6-30a746dd2054",
   "metadata": {},
   "source": [
    "### Normal Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce9bc9a-e5fd-457a-ae82-5a19e82acf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-5,5,0.01)     # range of values for z\n",
    "mu = 0                       # mu = 0 for standard normal\n",
    "sigma = 1                    # mu = 1 for standard normal\n",
    "\n",
    "# now calculate f(x)\n",
    "f = 1 / np.sqrt ((2 * np.pi * sigma ** 2)) * np.exp (-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "plt.plot(x,f,'k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b089b-9940-48c9-ae53-1f50a5791a9f",
   "metadata": {},
   "source": [
    "#### Seaborn Joint-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880a6647-6002-4211-b2c5-107a1cb2ef34",
   "metadata": {},
   "source": [
    "https://seaborn.pydata.org/generated/seaborn.jointplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffefb1f1",
   "metadata": {},
   "source": [
    "# *Classifcation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "y = df['insurance_claim']\n",
    "\n",
    "# features\n",
    "X = df.drop('insurance_claim', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da4dfb-1282-444d-beea-6f83bd8bf14b",
   "metadata": {},
   "source": [
    "#### Converting to dummy variables (i.e - transforming categorical features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the Features\n",
    "X_transformed = pd.get_dummies(X, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d535ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420945b-7921-40e9-84d9-d2332068694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1b10cb-0bfa-4adc-8be5-0132d713decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be45e4d6-ce68-4ecb-801c-7f452fa36ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4837db-8d27-4c11-b601-74756e9452ca",
   "metadata": {},
   "source": [
    "#### Intercept\n",
    "\n",
    "The interpretation of the parameters of the logistic model is not quite the same as for a linear regression model.\n",
    "\n",
    "In binary classification, the class with value 1 is known as the reference class. Let's explore.\n",
    "\n",
    "The intercept,  𝛽0 , is interpreted as the log odds ratio of an observation being in the reference class when all other predictor variables are equal to zero.\n",
    "\n",
    "We can exponentiate this value, in other words raise the natural number  𝑒  to this value, to convert it to a typical odds ratio. In other words:\n",
    "\n",
    "$$ 𝑂𝑑𝑑𝑠=𝑒𝛽0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833c2edf-5af9-47b5-abea-93a1cebb9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63697421-fa22-43a4-96ef-c8f11caf2723",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Coefficients\n",
    "\n",
    "For binary categorical variables, like smoker and sex, the coefficient is interpreted as the log odds ratio between the class implied by a zero for the variable (i.e. non-smoker), and the class implied by a one for the variable (i.e. smoker).\n",
    "\n",
    "For continuous variables, the coefficient is interpreted as the expected change in the log odds for a one-unit increase in the variable.\n",
    "\n",
    "Again, we can arrive at the usual odds value by exponentiating the coefficient:\n",
    "\n",
    "$$ 𝑂𝑑𝑑𝑠=𝑒𝛽1 $$\n",
    " \n",
    "Effectively, each coefficient is a measure of the change in the log odds of belonging to the reference class for one-unit changes in the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664e2dc-d6bb-4cf5-806a-14c467bd6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(lr.coef_.T, X_transformed.columns, columns=['Coefficient'])\n",
    "coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f795e6-72d2-4744-9bf6-3853d20a9b1c",
   "metadata": {},
   "source": [
    "## Statsmodels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e01a7f-406e-4caa-8a50-0d4a63fe1502",
   "metadata": {},
   "source": [
    "Code Examples: \n",
    "\n",
    " GeeksforGeeks: https://www.geeksforgeeks.org/logistic-regression-using-statsmodels/\n",
    "    \n",
    " Stackoverflow: https://stackoverflow.com/questions/61560569/simple-logistic-regression-with-statsmodels-adding-an-intercept-and-visualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666c71a-4715-44a4-a0ce-7851836b7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    " \n",
    "# loading the training dataset\n",
    "df = pd.read_csv('logit_train1.csv', index_col = 0)\n",
    " \n",
    "# defining the dependent and independent variables\n",
    "Xtrain = df[['gmat', 'gpa', 'work_experience']]\n",
    "ytrain = df[['admitted']]\n",
    "\n",
    "Xtrain = sm.add_constant(Xtrain)\n",
    "  \n",
    "# building the model and fitting the data\n",
    "log_reg = sm.Logit(ytrain, Xtrain).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e21ba4-1665-40a2-ba96-736dbaa953e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the summary table\n",
    "print(log_reg.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a5416-4619-49b2-812e-94e7ae545e4f",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edce2728-b371-4b4a-9e64-47f7980377b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614394b-c840-442c-b698-8726883aceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['species']\n",
    "X = df.drop('species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e7925-6f38-4e70-b73f-81f2d07ce158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarise the data\n",
    "standard_scaler = StandardScaler()\n",
    "X_transformed = standard_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed1ced-528d-434d-99e1-4a91099be8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.30, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9c158-d3e5-40e8-bad1-2122ac27397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9315c9-4bee-4bf0-820e-c878c36ac79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f4af2-8dcf-4e3d-a893-3619b79dce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03ba252-be1c-41a5-9458-d3ca59762b8d",
   "metadata": {},
   "source": [
    "Print how many of each class we have in this test set. Let's print that off before we print out the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81d94f-b5a4-4ab2-bcdd-accd4c479c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4b094-f5db-41b7-8d67-793b15252586",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Iris-setosa', 'Iris-versicolor','Iris-virginica']\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, y_pred), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc33efb-8b5b-4561-9101-5dac70c8941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=['Iris-setosa', 'Iris-versicolor','Iris-virginica']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d2d5ea-3dcc-4aec-850b-ffd5f1ea4c39",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c209a7-0021-45b5-a9e2-370d5280ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e9e0f-dde2-4c87-9bae-d0d1e7a76e48",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "Link: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc28ee0-f241-4c26-937d-7d24badb8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "start_time = time.time()\n",
    "result = permutation_importance(\n",
    "    forest, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
    "\n",
    "forest_importances = pd.Series(result.importances_mean, index=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5d5ce2-394f-4285-b152-d41854bc4ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['species']\n",
    "X = df.drop('species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8229c01-c9ea-460d-a0a9-b4d325c53e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standarise the data\n",
    "standard_scaler = StandardScaler()\n",
    "X_transformed = standard_scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f36c57c-e99c-40d0-b26e-ed633396c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.30, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0c3ef-2a6a-40ed-92d2-6f9d0186ba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aca60b-e13e-43e6-99d3-43340f486b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_forest = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038693b-91e6-440f-b21a-0b2385399195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred_forest, \n",
    "                            target_names=['Iris-setosa', 'Iris-versicolor','Iris-virginica']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f44ecdf-fc81-4ff1-b9fb-11bcf6e05982",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d8f5f-74ab-484a-a7ea-925613eb1e8d",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b43499-7fd5-4d6d-9992-18424ec84906",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81256dc6-46cf-4a65-8178-1fde0d7bd753",
   "metadata": {},
   "source": [
    "That doesn't look very nice - let's convert it into a dataframe and add the appropriate labels to make it clear which value is which.\n",
    "\n",
    "The matrix orders the rows and columns in a sorted fashion according to the labels. Our labels are 0 and 1, so the first row/column is 0, and the 2nd row/column is 1. Let's give it the appropriate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be611eea-0d86-4447-bd5f-c76369ef2250",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0: Malignant', '1: Benign']\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, pred_lm), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d39abe-0a95-4efe-89ac-790b7289fb3a",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eb26ec-b893-47d6-8196-54332c8c8f77",
   "metadata": {},
   "source": [
    "NOTE: 'C' is the penalty term. \n",
    "\n",
    "'RBF' = radial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9f169-91b3-433d-88c1-0222bb3cddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07cb0dc-c899-4c60-86c9-56520978e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642314c0-9b89-46c9-8fa6-3cc5f699c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(kernel='linear')\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(\"The accuracy score of the SVC is:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\n\\nClassification Report:\\n\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648871ed-bb76-4f2c-81ad-432afdb3a61e",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00418c8f-5d4d-49a4-9ff6-88d34c82c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983d83a-bc62-4b26-9c17-c3168310650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['species']\n",
    "X = df.drop('species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785959f-24e6-40f2-b28c-954e79c4399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 3 # <--- change this number to play around with how many nearest neighbours to look for.\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors)\n",
    "# Fit the model \n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0e862-5762-407e-8e8d-460e7dfaae5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 50, 100]\n",
    "\n",
    "results = []\n",
    "\n",
    "for k in ks:\n",
    "    print('Fitting KNN model with k = {:d}'.format(k))\n",
    "    knn = KNeighborsClassifier(k)\n",
    "    run_time = %timeit -q -o knn.fit(X_train, y_train)\n",
    "\n",
    "    # predicting\n",
    "    y_pred = knn.predict(X_train)\n",
    "    y_pred_test = knn.predict(X_test)\n",
    "\n",
    "    # scoring\n",
    "    accuracy  = metrics.accuracy_score(y_train, y_pred)\n",
    "    precision = metrics.precision_score(y_train, y_pred)\n",
    "    recall    = metrics.recall_score(y_train, y_pred)\n",
    "    f1        = metrics.f1_score(y_train, y_pred)\n",
    "    f1_test   = metrics.f1_score(y_test, y_pred_test)\n",
    "\n",
    "    # save the results \n",
    "    results.append([k, accuracy, precision, recall, f1, f1_test, run_time.best])\n",
    "\n",
    "results = pd.DataFrame(results, columns=['KNN', 'Accuracy', 'Precision', 'Recall', 'F1 Train', 'F1 Test', 'Train Time'])\n",
    "results.set_index('K', inplace= True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bcddd-29a5-483d-a805-19016ea82025",
   "metadata": {},
   "source": [
    "#### Assess model performance\n",
    "\n",
    "So far, we've been using the classification report and confusion matrices to assess classification model performance. However, such metrics don't do a good job at highlighting how confident our model is in its predictions.\n",
    "\n",
    "Enter the log loss function which, unlike other metrics, can penalise predictions based on how confident a model is with those predictions. For example, if our model predicts the wrong class with high probability, the log loss penalises it more (i.e.: assigns higher log loss) compared to a model that predicts the wrong class with low probability. As such, we generally feed class probabilities into the log loss function instead of the actual class predictions (i.e. thresholded probalities).\n",
    "\n",
    "For the log loss metric, lower is better, i.e.:, a perfect model would have a log loss of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c890d91-2283-472c-b2f1-13324b28cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set \n",
    "y_hat = knn.predict_proba(X_test)\n",
    "# Calculate the loss \n",
    "print(\"The log loss error for our model is: \", log_loss(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158caa8-ea07-4df3-9fba-ccac72104d31",
   "metadata": {},
   "source": [
    "## NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecfe9c1-7020-43f2-aea2-00be54aed0f2",
   "metadata": {},
   "source": [
    "dropna() : https://www.geeksforgeeks.org/python-pandas-dataframe-dropna/\n",
    "\n",
    "reset index : https://www.geeksforgeeks.org/python-pandas-dataframe-reset_index/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917beb1-ae28-4e8b-aece-85845a684ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286659dd-1615-4b5b-8321-9dc4de9ecff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important libraries\n",
    "\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# set plot style\n",
    "sns.set()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f912169f-c0fd-465c-85bb-0bc180fd63bc",
   "metadata": {},
   "source": [
    "*See supporting notebook (in repo) for individual text cleaning juices*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6773c7d-c552-429e-a95f-c4a51d8fe4b4",
   "metadata": {},
   "source": [
    "#### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920be1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    This function uses regular expressions to remove html characters,\n",
    "    punctuation, numbers and any extra white space from each text\n",
    "    and then converts them to lowercase.\n",
    "\n",
    "    Input:\n",
    "    text: original text\n",
    "          datatype: string\n",
    "\n",
    "    Output:\n",
    "    texts: modified text\n",
    "           datatype: string\n",
    "    \"\"\"\n",
    "    # replace the html characters with \" \"\n",
    "    text=re.sub('<.*?>', ' ', text)\n",
    "#    Removal of numbers\n",
    "#    text = re.sub(r'\\d+', ' ', text)\n",
    "    # will replace newline with space\n",
    "    text = re.sub(\"\\n\",\" \",text)\n",
    "    # will convert to lower case\n",
    "    text = text.lower()\n",
    "    # will split and join the words\n",
    "    text=' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the function to clean the tweets\n",
    "train['text'] = train['text'].apply(clean_text)\n",
    "test['text'] = test['text'].apply(clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95800dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '.txt' with 'text file'\n",
    "train[\"text\"] = train[\"text\"].str.replace(\".txt\", \" text file\")\n",
    "test[\"text\"] = test[\"text\"].str.replace(\".txt\", \" text file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc533a-06a1-49bb-a194-d30880a81e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet = \"first think another Disney movie, might good, it's kids movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove numbers\n",
    "    text_nonum = re.sub(r'\\d+', '', text)\n",
    "    # remove punctuations and convert characters to lower case\n",
    "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
    "    # substitute multiple whitespace with single whitespace\n",
    "    # Also, removes leading and trailing whitespaces\n",
    "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
    "    return text_no_doublespace\n",
    "\n",
    "cleaned_tweet = clean_text(tweet)\n",
    "tt = TweetTokenizer()\n",
    "print(tt.tokenize(cleaned_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Function for Model Building\n",
    "\n",
    "def models_building(classifiers, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    This function takes in a list of classifiers\n",
    "    and both the train and validation sets\n",
    "    and return a summary of F1-score and\n",
    "    processing time as a dataframe\n",
    "\n",
    "    Input:\n",
    "    classifiers: a list of classifiers to train\n",
    "                 datatype: list\n",
    "    X_train: independent variable for training\n",
    "             datatype: series\n",
    "    y_train: dependent variable for training\n",
    "             datatype: series\n",
    "    X_val: independent variable for validation\n",
    "           datatype: series\n",
    "    y_val: dependent variable for validation\n",
    "           datatype: series\n",
    "\n",
    "    Output:\n",
    "    model_summary: F1 Score for all the classifiers\n",
    "                   datatype: dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    models_summary = {}\n",
    "\n",
    "    # Pipeline to balance the classses and then to build the model\n",
    "    for clf in classifiers:\n",
    "        clf_text = Pipeline([('tfidf', TfidfVectorizer(min_df=1,\n",
    "                                                       max_df=0.9,\n",
    "                                                       ngram_range=(3, 6),\n",
    "                                                       analyzer='char')),\n",
    "                             ('clf', clf)])\n",
    "\n",
    "        # Logging the Execution Time for each model\n",
    "        start_time = time.time()\n",
    "        clf_text.fit(X_train, y_train)\n",
    "        predictions = clf_text.predict(X_val)\n",
    "        run_time = time.time()-start_time\n",
    "\n",
    "        # Output for each model\n",
    "        models_summary[clf.__class__.__name__] = {\n",
    "            'F1-Macro': metrics.f1_score(y_val,\n",
    "                                         predictions,\n",
    "                                         average='macro'),\n",
    "            'F1-Accuracy': metrics.f1_score(y_val, predictions,\n",
    "                                            average='micro'),\n",
    "            'F1-Weighted': metrics.f1_score(y_val,\n",
    "                                            predictions,\n",
    "                                            average='weighted'),\n",
    "            'Execution Time': run_time}\n",
    "\n",
    "    return pd.DataFrame.from_dict(models_summary, orient='index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed11eb3d",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning on Most Performing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2942f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pipeline for the gridsearch\n",
    "param_grid = {'alpha': [0.1, 1, 5, 10]}  # setting parameter grid\n",
    "\n",
    "HP_MNB = Pipeline([('tfidf', TfidfVectorizer(min_df=2,\n",
    "                                                max_df=0.9,\n",
    "                                                ngram_range=(3, 6),\n",
    "                                                analyzer='char')),\n",
    "                      ('mnb', GridSearchCV(MultinomialNB(),\n",
    "                                           param_grid=param_grid,\n",
    "                                           cv=5,\n",
    "                                           n_jobs=-1,\n",
    "                                           scoring='f1_weighted'))\n",
    "                      ])\n",
    "\n",
    "HP_MNB.fit(X_train, y_train)  # Fitting the model\n",
    "\n",
    "y_pred_mnb = HP_MNB.predict(X_val)  # predicting the fit on validation set\n",
    "\n",
    "print(classification_report(y_val, y_pred_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e02a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution of the Classifiers\n",
    "\n",
    "classifiers_df = models_building(classifiers, X_train, y_train, X_val, y_val)\n",
    "ordered_df = classifiers_df.sort_values('F1-Macro', ascending=False)\n",
    "ordered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5a100",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d01d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed32972",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4144a5ae",
   "metadata": {},
   "source": [
    "Pro-tip: in the multi-class case we referred to above, the LogisticRegression instance takes a simple argument which enables it to be used for 2+ classes: multi_class='ovr'. We'll revisit this later in the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff696e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10684b29",
   "metadata": {},
   "source": [
    "Advantages & Disadvantages of Logistic Regression\n",
    "Advantages\n",
    "\n",
    "Convenient probability scores for observations (probability of each outcome is transformed into a classification);\n",
    "Not a major issue if there is collinearity among features (much worse with linear regression).\n",
    "Disadvantages\n",
    "\n",
    "Can overfit when data is unbalanced (i.e.: we have far more observations in one class than the other);\n",
    "Doesn't handle large number of categorical variables well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f204fcb9",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4743641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, pred_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['0: Malignant', '1: Benign']\n",
    "\n",
    "pd.DataFrame(data=confusion_matrix(y_test, pred_lm), index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed15cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c639fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_test, pred_lm, target_names=['0: Malignant', '1: Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63a48dc",
   "metadata": {},
   "source": [
    "### Overall Accuracy\n",
    "\n",
    "The results shown above lead us to our first classification metric: **overall accuracy**, which we calculate according to the following formula:\n",
    "\n",
    "$$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions} = \\frac{TP + \\space TN}{TP \\space + \\space TN \\space + \\space FP \\space + \\space FN}$$\n",
    "\n",
    "Our overall accuracy is calculated as follows:\n",
    "\n",
    "$$Accuracy =  \\frac{Correct\\space predictions}{Total\\space predictions} = \\frac{70 + 36}{70 + 36 + 3 + 3} = 0.946$$\n",
    "\n",
    "At first glance this appears to a useful, catch-all metric which tells us everything we need to know about our model. The problem is that it lacks detail.\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "- We have 100 observations in our test dataset: 90 of them are labelled _No_ , the remaining 10 labelled _Yes_. \n",
    "\n",
    "- At prediction time, our model classifies all 100 observations to be in category _No_. Our model made 100 predictions, and got all 90 of the _No_ observations correct, giving it an overall accuracy of 90%!\n",
    "\n",
    "- Sounds good right? The problem is that the model got literally none of the _Yes_-labelled observations correct - 0/10! What if the _Yes_ cases were for patients have cancer, or a transaction that is fraudulent? Those are important results, and we would have missed all of them.\n",
    "\n",
    "- Hopefully, that has highlighted the importance of being accurate not just overall, but in each particular class too.\n",
    "\n",
    "Let's look at few metrics which are a little more comprehensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e64ac",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "\n",
    "When it predicts _yes_, how often is it correct? \n",
    "\n",
    "$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6d7d0",
   "metadata": {},
   "source": [
    "#### Recall\n",
    "\n",
    "When the outcome is actually _yes_, how often do we predict it as such?\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192c4c6",
   "metadata": {},
   "source": [
    "#### F1 Score\n",
    "\n",
    "Weighted average of precision and recall. \n",
    "\n",
    "$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c252c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aae258",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_test, pred_lm, target_names=['0: Malignant', '1: Benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c77c3-5485-4136-b0fe-405a0d4333f4",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab46f1-05a3-4465-99c4-8c01ed0e60f6",
   "metadata": {},
   "source": [
    "Get most common value in column of Pandas DataFrame: https://stackoverflow.com/questions/48590268/pandas-get-the-most-frequent-values-of-a-column/48590361"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d645f80-6b1f-45ea-8f29-d3c0c3ef2b8f",
   "metadata": {},
   "source": [
    "Subset Pandas Data: https://www.geeksforgeeks.org/how-to-select-a-subset-of-a-dataframe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef555ba-eccd-4de1-980a-6ac39cc239f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f01d1-c724-41ce-8397-f9c661e68e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ace97-a55e-4902-9e1b-1dea95ef13cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
